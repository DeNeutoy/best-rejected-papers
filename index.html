<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v2.js"></script>
<script src="https://d3js.org/d3.v7.min.js"></script>
<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<script src="data/top_10_per_conf.js"></script>
<script src="data/high_impact_first_authors_rejected.js"></script>
<script src="best-rejected-papers.js" defer></script>
<script src="high-impact-first-authors.js" defer></script>
<link rel="stylesheet" href="styles.css">
<link rel="stylesheet" href="best-rejected-papers.css">
<link rel="stylesheet" href="high-impact-first-authors.css">

<!-- OpenGraph Meta Tags -->
<meta property="og:title" content="The Best Rejected Papers">
<meta property="og:description" content="OpenReview's radical transparency allows us to look at the quality of peer review over time. Discover the most highly cited papers that were initially rejected from top ML conferences.">
<meta property="og:image" content="./og-image.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:image:alt" content="Data visualization showing citation counts vs review scores for rejected papers">
<meta property="og:url" content="https://deneutoy.github.io/best-rejected-papers/">
<meta property="og:type" content="article">
<meta property="og:site_name" content="The Best Rejected Papers">

<!-- Twitter Card Meta Tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="The Best Rejected Papers">
<meta name="twitter:description" content="OpenReview's radical transparency allows us to look at the quality of peer review over time. Discover the most highly cited papers that were initially rejected from top ML conferences.">
<meta name="twitter:image" content="./images/og-image.png">

<!-- Fragments -->
<script src="fragments/paper_matching_success_rate.js"></script>
<script src="fragments/failed_papers_by_conference.js"></script>
<script src="fragments/citation_count_vs_review_score.js"></script>
<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
    "title": " The Best Rejected Papers",
    "description": "OpenReview's radical transparency allows us to look at the quality of peer review over time.",
    "published": "Feb 19, 2025",
    "doi": "TBC",
    "authors": [
      {
        "author": "Mark Neumann",
        "authorURL": "https://markneumann.xyz",
        "affiliation":"markneumann.xyz",
        "affiliationURL":"https://markneumann.xyz"
      }
    ],
    "katex": {
      "delimiters" : [
        {"left": "$", "right": "$", "display": false}
      ]
    }
  }</script>
</d-front-matter>

<d-title>
  <h1>The Best Rejected Papers</h1>
</d-title>
<d-byline></d-byline>


<d-article>

  <h2>What is OpenReview?</h2>
  <p>
    <a href="https://openreview.net/" target="_blank">OpenReview</a> is a platform designed to
    encourage openness and public access in scientific communication, with a particular emphasis
    on the peer review process.
    It provides an <a href="https://github.com/openreview" target="_blank">open source platform</a> 
    for conferences to manage their review process. OpenReview has been used by many large Machine Learning conferences
    to manage workshops, full conference submissions and even longer running editorial review processes. 
    OpenReview.net <d-cite key="Soergel2013OpenSA"></d-cite> was created by Andrew McCallum's Information 
    Extraction and Synthesis Laboratory at UMass Amherst. 
  </p>

  <p>
   One particularly interesting feature of conference submissions using OpenReview is the
   publication of paper acceptance decisions, official reviews and fully anonymized 
   discussions surrounding paper submissions. 
  </p>

  <p>
    As these acceptance decisions are public, we can look at the quality of peer review over time (and other interesting patterns)
    by linking OpenReview submissions to external academic graphs (in this case, <a href="https://semanticscholar.org/" target="_blank">Semantic Scholar</a>).
  </p>


  <h2>The Data</h2>
  
  <p>
    Many conferences use OpenReview to manage parts of their conference process (in particular, for workshop reviewing).
    Two conferences have used OpenReview consistently to manage their main conference tracks - 
    <a href="https://openreview.net/group?id=ICLR.cc/2013/Conference" target="_blank">The International Conference on Learning Representations (ICLR)</a> and 
    <a href="https://openreview.net/group?id=NeurIPS.cc/2017/Conference" target="_blank">Neural Information Processing Systems (NeurIPS)</a>.

  In particular, we will focus on the following conference years:
  <ul>
    <li> ICLR: 2017, 2019-2025
    <li> Neurips: 2021-2025
  </ul>

  <p>
    Neurips only transitioned to OpenReview in 2021, and for 2018, OpenReview was used for ICLR reviewing but the decisions and reviews were not published.
    There is also one important difference between the two conferences - all papers submitted to ICLR are public by default, but papers rejected from Neurips are  
    only made public with the author's permission. This introduces a clear bias in the data (which can be seen in the number of rejected papers below).
  </p>


  </p>

  <h2>Paper Matching Success Rate</h2>

  <p>
    To link OpenReview papers to their Semantic Scholar papers, we used the <a href="https://api.semanticscholar.org/api-docs/graph" target="_blank">Semantic Scholar Graph API</a>.
    As a first step, I just used exact title matching to link papers, which is reasonably effective. 
    The largest source of errors when using this approach came from papers with LaTeX in the title, 
    as well as as papers which changed their title from a pre-print version which was already publicly available.
  </p>

  <p>
    The following chart shows the success rate of this approach over time.
  </p>

    <div id="4b284f59-68f8-4606-9df1-192afcf39a27" class="l-body-outset" style="height:500px; width:800px;"></div>


    <p>
      To catch some of these errors, I then used a second pass to approximately link papers. This used Semantic Scholar's Paper Search API to retrieve
      the top 10 search results for a given paper title. I then checked for the highest scoring result based on the following criteria:

      <ul>
        <li>
          Levenshtein distance between titles
        </li>
        <li>
          Average Levenshtein distance between authors, ordered
        </li>
        <li>
          Discarding results where the highest ranking result based on the above criteria had a title similarity of less than 0.7
        </li>
      </ul>

      <p>
      This catches quite a few of the errors (particularly related to syntactical differences in the title). For some papers submitted
      to OpenReview, there actually is no publicly available paper, typically because these papers are either improved for another subsequent
      conference, or are not high enough quality to be published.
</p>
    </p>

    <p>
      To check this, we can look at the acceptance rate of papers which were not matched to a publicly available paper. As the graph below shows,
      most papers across all conferences which were not linkable were either rejected or withdrawn.
    </p>

    <div id="92dddb17-1483-4db3-afc8-0e627a1d8080" class="l-body-outset" style="height: 500px; width: 800px" ></div>

      <p>
        The reward for doing this work is a set of 38,262 linked records between OpenReview submissions and a dynamic academic graph, as well as 462,995 structured comments from reviewers.
        <a href="https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_get_paper" target="_blank">Semantic Scholar's paper information</a> 
        is quite rich, and includes information about authors, citations, and a variety of other metadata. By default, I added
        citation counts, venue information, Specter embeddings <d-cite key="Singh2022SciRepEvalAM"></d-cite> and author ids to the dataset - but other information is easily retrievable using the <d-code language="python">semantic_scholar_id</d-code>.


        This gives us the ability to look at the quality of peer review over time (and also means this dataset can be updated in the future, as papers
        gather more citations).
      </p>
    <div class="l-body-outset">
    <d-code block language="python">
      id                                                               BkbY4psgg
      semantic_scholar_id               6b024162f81e8ff7aa34c3a43d601a912d012c78
      raw_decision                                                ICLR 2017 Oral
      normalized_decision                                                   Oral
      title                    Making Neural Programming Architectures Genera...
      abstract                 Empirically, neural networks that attempt to l...
      keywords                                                   [Deep learning]
      accepted                                                              True
      publication_venue        International Conference on Learning Represent...
      publication_venue_id                  939c6e1d-0d17-4d6e-8a82-66d960df0e40
      url                      https://www.semanticscholar.org/paper/6b024162...
      citation_count                                                         146
      embedding                [-0.0735881552, 0.3261716962, -0.3699628115, -...
      authors                              [Jonathon Cai, Richard Shin, D. Song]
      authorIds                                   [2350111, 39428234, 143711382]
      conference_year                                                       2017
      conference_name                                                       iclr
      conf_id                                                           iclr2017
      review_scores                                              [8.0, 9.0, 8.0]
      review_score_avg                                                  8.333333
      review_confidences                                         [8.0, 9.0, 8.0]
      review_confidence_avg                                                  4.0
    </d-code>
    <div style="text-align: center;">
      <i style="font-size: 12px;">
        An example record from the dataset.
      </i>
    </div>
    </div>

    In addition to the paper information, we also have a set of 462,995 structured comments from reviewers. these
    comments include references to papers, anonymous author signatures, and arbitrary content (typically in the
    form of title:content blocks which render in OpenReview). Reviews are distingushed from other comments by the
    'is_review' field, which is set to True for reviews. These comments will also have a numeric rating and confidence score.
    Finally, all comments have a reply_to_id field, which links to the id of the paper that the comment is replying to. Review comments
    will have a reply_to_id that links to the id of the paper they are reviewing.  <d-footnote>There are a small number of orphaned comments which do not have a reply_to_id. From inspection, these tend to be special cases where an AC has manually commented on a paper conversation.</d-footnote>


    <div class="l-body-outset">
      <d-code block language="javascript">
        {'conference_id': 'iclr2017',
        'paper_id': 'B1jnyXXJx',
        'comment_id': 'BJPZL-vmx',
        'signature': 'ICLR.cc/2017/conference/paper4/AnonReviewer1',
        'content': {
          'title': 'hyperparameter optimization and momentum vs CPN',
          'question': "The hyperparameters of gradient descent seem to be chosen once and fixed. 
                       Would optimizing the gradient descent hyperparameters lead to equivalent 
                       performance as the CPN method?\n\nFollowing up on another reviewer's 
                       question: CPN seems closely related to momentum. Can you provide a clear 
                       example to show how CPN is qualitatively distinct from momentum? (I believe
                       it is, but this could be clarified further in the paper)"
          },
        'reply_to_id': 'B1jnyXXJx',
        'is_review': False,
        'rating': None,
        'numeric_rating': None,
        'confidence': None,
        'numeric_confidence': None}
      </d-code>
      <div style="text-align: center;">
        <i style="font-size: 12px;">
          An example record from the dataset.
        </i>
      </div>
      </div>



    </p>

    <div id="98482921-8bd7-431e-bab8-521dcde1ff79" class="l-screen-inset" style="height: 1600px; width: 1200px; margin: 0 auto" ></div>


  <h2>🏆 Best Rejected Papers 🏆</h2>
  <p>Below are the most highly cited papers that were initially rejected from these conferences, organized by venue:</p>
  
  <p>
  <div id="best-rejected-visualization" style="margin-top: 20px; margin-bottom: 20px;">
    <div class="conference-tabs">
      <!-- Tab buttons will be dynamically generated based on available data -->
    </div>
    
    <div class="papers-container">
      <div id="papers-loading" class="loading-message">Loading papers...</div>
      <div id="papers-content" class="papers-content"></div>
    </div>
  </div>
  </p>




  <h2> High Impact, "Unlucky" First Authors </h2>

  <p>
    The following shows the most productive first authors whose papers were initially rejected from these conferences. 
    Each author is ranked by their average citations per year, with expandable details showing their rejected papers and citation counts.
  </p>

  <div id="high-impact-first-author-rejected-visualization" style="margin-top: 20px; margin-bottom: 20px;">
  </div>

  <p>This is a footnote <d-footnote>This will become a hoverable footnote.</d-footnote></p>

</d-article>

<d-appendix>
  <d-bibliography>
    <script type="text/bibtex">
      @article{gregor2015draw,
        title={DRAW: A recurrent neural network for image generation},
        author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
        journal={arXivreprint arXiv:1502.04623},
        year={2015},
        url={https://arxiv.org/pdf/1502.04623.pdf}
      }
      @inproceedings{Soergel2013OpenSA,
        title={Open Scholarship and Peer Review: a Time for Experimentation},
        author={David Soergel and Adam Saunders and Andrew McCallum},
        year={2013},
        url={https://api.semanticscholar.org/CorpusID:14548845}
      }
      @inproceedings{Singh2022SciRepEvalAM,
        title={SciRepEval: A Multi-Format Benchmark for Scientific Document Representations},
        author={Amanpreet Singh and Mike D'Arcy and Arman Cohan and Doug Downey and Sergey Feldman},
        booktitle={Conference on Empirical Methods in Natural Language Processing},
        year={2022},
        url={https://api.semanticscholar.org/CorpusID:254018137}
      }
    </script>
  </d-bibliography>
  <style>
      d-appendix .citation {
          font-size: 11px;
          line-height: 15px;
          border-left: 1px solid rgba(0, 0, 0, 0.1);
          padding-left: 18px;
          border: 1px solid rgba(0, 0, 0, 0.1);
          background: rgba(0, 0, 0, 0.02);
          padding: 10px 18px;
          border-radius: 3px;
          color: rgba(150, 150, 150, 1);
          overflow: hidden;
          margin-top: -12px;
          white-space: pre-wrap;
          word-wrap: break-word;
      }
  </style>

  <h3 id="citation">Citation</h3>
  <p>For attribution in academic contexts, please cite this work as</p>
  <pre
      class="citation short">"The Distill Blog Template", 2025.</pre>
  <p>BibTeX citation</p>
  <pre class="citation long">@misc{distill_blog_template,
title={The Distill Blog Template},
author={Some Authors et al},
year={2025},
}</pre>
</d-appendix>
