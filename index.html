<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
  <title>The Best Rejected Papers</title>
<script src="https://distill.pub/template.v2.js"></script>
<script src="https://d3js.org/d3.v7.min.js"></script>
<script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>
<script src="data/top_10_per_conf.js"></script>
<script src="data/high_impact_first_authors_rejected.js"></script>
<script src="best-rejected-papers.js" defer></script>
<script src="high-impact-first-authors.js" defer></script>
<link rel="stylesheet" href="styles.css">
<link rel="stylesheet" href="best-rejected-papers.css">
<link rel="stylesheet" href="high-impact-first-authors.css">

<!-- OpenGraph Meta Tags -->
<meta property="og:title" content="The Best Rejected Papers">
<meta property="og:description" content="OpenReview's radical transparency allows us to look at the quality of peer review over time. Discover the most highly cited papers that were initially rejected from top ML conferences.">
<meta property="og:image" content="og-image.png">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="630">
<meta property="og:image:alt" content="Data visualization showing citation counts vs review scores for rejected papers">
<meta property="og:url" content="https://deneutoy.github.io/best-rejected-papers/">
<meta property="og:type" content="article">
<meta property="og:site_name" content="The Best Rejected Papers">

<!-- Twitter Card Meta Tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="The Best Rejected Papers">
<meta name="twitter:description" content="OpenReview's radical transparency allows us to look at the quality of peer review over time. Discover the most highly cited papers that were initially rejected from top ML conferences.">
<meta name="twitter:image" content="./images/og-image.png">

<!-- Fragments -->
<script src="fragments/paper_matching_success_rate.js"></script>
<script src="fragments/failed_papers_by_conference.js"></script>
<script src="fragments/citation_count_vs_review_score.js"></script>
<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
    "title": " The Best Rejected Papers",
    "description": "OpenReview's radical transparency allows us to look at the quality of peer review over time.",
    "published": "July 13, 2025",
    "doi": "10.5281/zenodo.15866613",
    "authors": [
      {
        "author": "Mark Neumann",
        "authorURL": "https://markneumann.xyz",
        "affiliation":"markneumann.xyz",
        "affiliationURL":"https://markneumann.xyz"
      }
    ],
    "katex": {
      "delimiters" : [
        {"left": "$", "right": "$", "display": false}
      ]
    }
  }</script>
</d-front-matter>

</head>
<d-title>
  <h1>The Best Rejected Papers</h1>
</d-title>
<d-byline></d-byline>


<d-article style="overflow-x: scroll;">
  <p>
    Peer review is the cornerstone of scientific publishing, yet its effectiveness remains a subject of ongoing debate,
    particularly as conference submission numbers continue to grow.
    This blog post introduces the Structured, Normalized OpenReview dataset (SNOR v1), a dataset derrived from <a href="https://openreview.net/" target="_blank">OpenReview</a>, 
    a platform that provides unprecedented transparency into the peer review process for two of the largest Machine
    Learning conferences. By examining rejected papers from top machine learning conferences but went on to achieve high 
    citation counts, we can gain insights into the challenges and limitations of academic peer review.
  </p>
  <p>
    The dataset is available under the <a href="https://creativecommons.org/licenses/by/4.0/deed.en" target="_blank">CC-BY 4.0 license</a> 
    and is available <a href="https://zenodo.org/records/15866613" target="_blank">on Zenodo</a>.
  </p>

  <h2>What is OpenReview?</h2>
  <p>
    <a href="https://openreview.net/" target="_blank">OpenReview</a> is a platform designed to
    encourage openness and public access in scientific communication, with a particular emphasis
    on the peer review process.
    It provides an <a href="https://github.com/openreview" target="_blank">open source platform</a> 
    for conferences to manage their review process. OpenReview has been used by many large Machine Learning conferences
    to manage workshops, full conference submissions and even longer running editorial review processes. 
    OpenReview.net <d-cite key="Soergel2013OpenSA"></d-cite> was created by Andrew McCallum's Information 
    Extraction and Synthesis Laboratory at UMass Amherst. 
  </p>

  <p>
   One particularly interesting feature of conference submissions using OpenReview is the
   publication of paper acceptance decisions, official reviews and fully anonymized 
   discussions surrounding paper submissions. 
  </p>

  <p>
    As these acceptance decisions are public, we can look at the quality of peer review over time (and other interesting patterns)
    by linking OpenReview submissions to external academic graphs (in this case, <a href="https://semanticscholar.org/" target="_blank">Semantic Scholar</a>).
  </p>


  <h2>The Data</h2>
  
  <p>
    Many conferences use OpenReview to manage parts of their conference process (in particular, for workshop reviewing).
    Two conferences have used OpenReview consistently to manage their main conference tracks - 
    <a href="https://openreview.net/group?id=ICLR.cc/2013/Conference" target="_blank">The International Conference on Learning Representations (ICLR)</a> and 
    <a href="https://openreview.net/group?id=NeurIPS.cc/2017/Conference" target="_blank">Neural Information Processing Systems (NeurIPS)</a>.

  In particular, we will focus on the following conference years:
  <ul>
    <li> ICLR: 2017, 2019-2025
    <li> Neurips: 2021-2025
  </ul>

  <p>
    Neurips only transitioned to OpenReview in 2021, and for 2018, OpenReview was used for ICLR reviewing but the decisions and reviews were not published.
    There is also one important difference between the two conferences - all papers submitted to ICLR are public by default, but papers rejected from Neurips are  
    only made public with the author's permission. This introduces a clear bias in the data (which can be seen in the number of rejected papers below).
  </p>


  </p>


  <p>  
    When papers are submitted to a conference on OpenReview, they are anonymized for the duration of the review process.
    Multiple reviews from official reviewers, Area Chairs and interested
    members of the public are published alongside the paper. These reviews are also anonymized, and paper authors are not able to see reviewer identities. Official reviews
    include structured metadata about the review (in particlar, a numeric score and confidence rating).
  </p>
  <img src="review.png" alt="Review" style="width: 100%; height: auto;">
  <div class="caption">
      An example review from ICLR 2025. Papers are reviewed by multiple reviewers, and the review scores and confidence ratings are published alongside the paper.
  </div>


  <p>
    All of this data is super interesting, but it is not in a very usable format - ideally we need to link paper submissions to papers which have made it out into the real world,
    so we can see how reviews, scores and acceptance decisions correlate with citations.
  </p>

  <h2>Paper Matching Success Rate</h2>

  <p>
    OpenReview papers are linked to their Semantic Scholar papers using the <a href="https://api.semanticscholar.org/api-docs/graph" target="_blank">Semantic Scholar Graph API</a>.
    As a first step, papers are linked using exact title matching, which is reasonably effective - covering about 80% of papers.
    The largest source of errors when using this approach came from papers with LaTeX in the title, 
    as well as as papers which changed their title from a pre-print version which was already publicly available.
  </p>

  <p>
    The following chart shows the success rate of this approach per conference.
  </p>

    <d-figure style="grid-column: screen; margin: 0 auto;">
      <div id="4b284f59-68f8-4606-9df1-192afcf39a27" style="height:500px; width:800px;"></div>
    </d-figure>


    <p>
      To catch some of these errors, I then used a second pass to approximately link papers. This used Semantic Scholar's Paper Search API to retrieve
      the top 10 search results for a given paper title. I then checked for the highest scoring result based on the following criteria:

      <ul>
        <li>
          Levenshtein distance between titles
        </li>
        <li>
          Average Levenshtein distance between authors, ordered paper author order
        </li>
        <li>
          Discarding results where the highest ranking result based on the above criteria had a title similarity of less than 0.7
        </li>
      </ul>

      <p>
      This catches quite a few of the errors (particularly related to syntactic differences in the title). For some papers submitted
      to OpenReview, there actually is no publicly available paper, typically because these papers are either improved for another subsequent
      conference, or are not high enough quality to be published.
</p>
    </p>

    <p>
      To check this, we can look at the acceptance rate of papers which were not matched to a publicly available paper. As the graph below shows,
      most papers across all conferences which were not linkable were either rejected or withdrawn.
    </p>

    <d-figure style="grid-column: screen; margin: 0 auto;">
      <div id="92dddb17-1483-4db3-afc8-0e627a1d8080" style="height: 500px; width: 800px" ></div>
    </d-figure>

      <p>
        The reward for doing this work is SNOR v1, a dataset of 38,262 linked records between OpenReview submissions and a dynamic academic graph, as well as 462,995 structured comments from reviewers.
        <a href="https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_get_paper" target="_blank">Semantic Scholar's paper information</a> 
        is quite rich, and includes information about authors, citations, and a variety of other metadata. By default, I added
        citation counts, venue information, Specter embeddings <d-cite key="Singh2022SciRepEvalAM"></d-cite> and author ids to the dataset - but other information is easily retrievable using the <d-code language="python">semantic_scholar_id</d-code>.


        This gives us the ability to look at the quality of peer review over time (and also means this dataset can be updated in the future, as papers
        gather more citations).
      </p>
    <div class="l-body-outset">
    <d-code block language="python">
      id                                                               BkbY4psgg
      semantic_scholar_id               6b024162f81e8ff7aa34c3a43d601a912d012c78
      raw_decision                                                ICLR 2017 Oral
      normalized_decision                                                   Oral
      title                    Making Neural Programming Architectures Genera...
      abstract                 Empirically, neural networks that attempt to l...
      keywords                                                   [Deep learning]
      accepted                                                              True
      publication_venue        International Conference on Learning Represent...
      publication_venue_id                  939c6e1d-0d17-4d6e-8a82-66d960df0e40
      url                      https://www.semanticscholar.org/paper/6b024162...
      citation_count                                                         146
      embedding                [-0.0735881552, 0.3261716962, -0.3699628115, -...
      authors                              [Jonathon Cai, Richard Shin, D. Song]
      authorIds                                   [2350111, 39428234, 143711382]
      conference_year                                                       2017
      conference_name                                                       iclr
      conf_id                                                           iclr2017
      review_scores                                              [8.0, 9.0, 8.0]
      review_score_avg                                                  8.333333
      review_confidences                                         [8.0, 9.0, 8.0]
      review_confidence_avg                                                  4.0
    </d-code>
    <div style="text-align: center;">
      <i style="font-size: 12px;">
        An example record from the dataset.
      </i>
    </div>
    </div>

    <p>
    In addition to the paper information, we also have a set of 462,995 structured comments from reviewers. these
    comments include references to papers, anonymous author signatures, and arbitrary content (typically in the
    form of title:content blocks which render in OpenReview). Reviews are distingushed from other comments by the
    'is_review' field, which is set to True for reviews. These comments will also have a numeric rating and confidence score.
    Finally, all comments have a reply_to_id field, which links to the id of the paper that the comment is replying to. Review comments
    will have a reply_to_id that links to the id of the paper they are reviewing.  <d-footnote>There are a small number of orphaned comments which do not have a reply_to_id. From inspection, these tend to be special cases where an AC has manually commented on a paper conversation.</d-footnote>
    </p>

    <div class="l-body-outset">
      <d-code block language="javascript">
        {'conference_id': 'iclr2017',
        'paper_id': 'B1jnyXXJx',
        'comment_id': 'BJPZL-vmx',
        'signature': 'ICLR.cc/2017/conference/paper4/AnonReviewer1',
        'content': {
          'title': 'hyperparameter optimization and momentum vs CPN',
          'question': "
              The hyperparameters of gradient descent seem to be chosen once and fixed. 
              Would optimizing the gradient descent hyperparameters lead to equivalent 
              performance as the CPN method?\n\nFollowing up on another reviewer's 
              question: CPN seems closely related to momentum. Can you provide a clear 
              example to show how CPN is qualitatively distinct from momentum? (I believe
              it is, but this could be clarified further in the paper)"
          },
        'reply_to_id': 'B1jnyXXJx',
        'is_review': False,
        'rating': None,
        'numeric_rating': None,
        'confidence': None,
        'numeric_confidence': None}
      </d-code>
      <div class="caption">
          An example comment from the dataset.
      </div>
      </div>
    </p>


    <h2>So - are we any good at reviewing?</h2>

    <p>
      Now we have OpenReview submissions to these conferences aligned with a academic graph, we can compare
      citation counts to review scores - do accepted papers correlate with more citations? Overall, yes! In general
      across conferences, accepted papers have a higher citation count than rejected papers. However, this bad news is that
      it does look like we are getting worse at reviewing over time - ICLR 2024/5 have a larger number of rejected papers with
      a substantial number of citations. <d-footnote> Of course, papers can be influential for many reasons, and some of these
        reasons are not related to scientific novelty. So this correlation is not a perfect proxy for review quality.</d-footnote>
    </p>

    <d-figure style="grid-column: screen;">
        <div id="98482921-8bd7-431e-bab8-521dcde1ff79" style="height: 1600px; width: 1200px; margin: 0 auto" ></div>
    </d-figure>

  <h2>🏆 Best Rejected Papers 🏆</h2>
  <p>
    One of the most interesting things about this dataset is the ability to look at the most highly cited papers that were initially rejected from these conferences. 
    Some extremely influential papers were initially rejected from these conferences - for example, ROBERTA <d-cite key="Liu2019RoBERTaAR"></d-cite> was rejected from ICLR 2020, and has subsequently been cited over 20,000 times.
  </p>
  
  <p>
  <div id="best-rejected-visualization" style="margin-top: 20px; margin-bottom: 20px;">
    <div class="conference-tabs">
      <!-- Tab buttons will be dynamically generated based on available data -->
    </div>
    
    <div class="papers-container">
      <div id="papers-loading" class="loading-message">Loading papers...</div>
      <div id="papers-content" class="papers-content"></div>
    </div>
  </div>
  </p>


  <h2> High Impact, "Unlucky" First Authors </h2>

  <p>
    By linking papers to an academic graph, we can also link paper authors to their academic graph profiles. This allows
    us to look at interesting patterns, like the most "unlucky" first authors. 
    The following interactive visualization shows the most productive first authors whose papers were rejected from these conferences, but were highly cited on average.
    Each author is ranked by their average citations per year, with expandable details showing their rejected papers and citation counts.
  </p>

  <div id="high-impact-first-author-rejected-visualization" style="margin-top: 20px; margin-bottom: 20px;">
  </div>



  <h2>Conclusion & Future Work</h2>
  <p>
    SNOR v1 is not the first attempt to use peer review data to better understand the scientific publishing process. The AMPERE dataset<d-cite key="hua-etal-2019-argument"></d-cite>
    approaches peer reviews through the lens of argument mining, focusing on the identification and classification of argumentative propositions. 
    Their annotated dataset of 400 reviews reveals patterns in how reviewers structure their feedback, offering insights into proposition types such as evaluation, requests, and factual statements.
    Similarly, COMPARE <d-cite key="9651878"></d-cite> designed a taxonomy and dataset of comparison discussions in peer reviews.
   </p> 
    <p>
    PeerRead <d-cite key="kang-etal-2018-dataset"></d-cite> was one of the first larger-scale public datasets of peer reviews, comprising  14.7K paper drafts and 10.7K reviews paired with paper drafts and acceptance decisions.
    CiteTracked <d-cite key="Plank2019CiteTrackedAL"></d-cite> adds a longitudinal dimension by linking peer reviews to citation statistics across six years of machine learning publications.
     
    </p>

    <p>
      SNOR v1 differs from these resources in a several ways. Firstly, it is substantially larger, comprising 462,995 comments on 38,262 papers.
      Secondly, SNOR v1 provides structured comment metadata including replies, allowing for more complex analyses involving review discussions, rebuttals and other interactions.
      Finally, SNOR v1 normalizes review metadata to provide consistent numeric scores and confidence ratings, allowing for more accurate comparisons between reviews. Links to
      an academic graph which not only include paper identifiers but also normalized author and venue identifiers allow a wider range of analyses than previous datasets.
    </p>

  <p>
    This dataset is a first step towards a more comprehensive understanding of the peer review process.
    I hope it will be useful to others, and I welcome any feedback or suggestions for how to improve it. There are several flaws currently - in particular,
    this is a static snapshot of a fixed set of conferences at a single point in time. As papers are updated and cited, this dataset will need to be updated.
    Additionally, as more conferences adopt OpenReview, this dataset will need to be updated to include those conferences. 
    For example, <a href="https://openreview.net/group?id=ICML.cc/2025/Conference" target="_blank">ICML 2025 followed the
    same fully open review process this year</a>, and this dataset does not currently include those papers.
  </p>

  <p>
    Despite these limitations, SNOR v1's features should allow a wide range of analyses and uses across both science of science, as well as a textual resource.
    Given the recent interest in Large Language Models for scientific authorship and reviewing, SNOR's structured comment metadata could be an invaluable dataset
    for reinforcement learning from human feedback - I'm excited to see what others will build with it!
  </p>
</d-article>


<d-appendix>
  <d-bibliography>
    <script type="text/bibtex">
      @inproceedings{Soergel2013OpenSA,
        title={Open Scholarship and Peer Review: a Time for Experimentation},
        author={David Soergel and Adam Saunders and Andrew McCallum},
        year={2013},
        url={https://api.semanticscholar.org/CorpusID:14548845}
      }
      @inproceedings{Singh2022SciRepEvalAM,
        title={SciRepEval: A Multi-Format Benchmark for Scientific Document Representations},
        author={Amanpreet Singh and Mike D'Arcy and Arman Cohan and Doug Downey and Sergey Feldman},
        booktitle={Conference on Empirical Methods in Natural Language Processing},
        year={2022},
        url={https://api.semanticscholar.org/CorpusID:254018137}
      }
      @inproceedings{Plank2019CiteTrackedAL,
        title={CiteTracked: A Longitudinal Dataset of Peer Reviews and Citations},
        author={Barbara Plank and Reinard van Dalen},
        booktitle={BIRNDL@SIGIR},
        year={2019},
        url={https://api.semanticscholar.org/CorpusID:198489688}
      }
      @inproceedings{hua-etal-2019-argument,
          title = "Argument Mining for Understanding Peer Reviews",
          author = "Hua, Xinyu  and
            Nikolov, Mitko  and
            Badugu, Nikhil  and
            Wang, Lu",
          editor = "Burstein, Jill  and
            Doran, Christy  and
            Solorio, Thamar",
          booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
          month = jun,
          year = "2019",
          address = "Minneapolis, Minnesota",
          publisher = "Association for Computational Linguistics",
          url = "https://aclanthology.org/N19-1219/",
          doi = "10.18653/v1/N19-1219",
          pages = "2131--2137",
          abstract = "Peer-review plays a critical role in the scientific writing and publication ecosystem. To assess the efficiency and efficacy of the reviewing process, one essential element is to understand and evaluate the reviews themselves. In this work, we study the content and structure of peer reviews under the argument mining framework, through automatically detecting (1) the argumentative propositions put forward by reviewers, and (2) their types (e.g., evaluating the work or making suggestions for improvement). We first collect 14.2K reviews from major machine learning and natural language processing venues. 400 reviews are annotated with 10,386 propositions and corresponding types of Evaluation, Request, Fact, Reference, or Quote. We then train state-of-the-art proposition segmentation and classification models on the data to evaluate their utilities and identify new challenges for this new domain, motivating future directions for argument mining. Further experiments show that proposition usage varies across venues in amount, type, and topic."
      }
      @inproceedings{kang-etal-2018-dataset,
        title = "A Dataset of Peer Reviews ({P}eer{R}ead): Collection, Insights and {NLP} Applications",
        author = "Kang, Dongyeop  and
          Ammar, Waleed  and
          Dalvi, Bhavana  and
          van Zuylen, Madeleine  and
          Kohlmeier, Sebastian  and
          Hovy, Eduard  and
          Schwartz, Roy",
        editor = "Walker, Marilyn  and
          Ji, Heng  and
          Stent, Amanda",
        booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
        month = jun,
        year = "2018",
        address = "New Orleans, Louisiana",
        publisher = "Association for Computational Linguistics",
        url = "https://aclanthology.org/N18-1149/",
        doi = "10.18653/v1/N18-1149",
        pages = "1647--1661",
        abstract = "Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21{\%} error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as `originality' and `impact'."
    }
    @INPROCEEDINGS{9651878,
      author={Singh, Shruti and Singh, Mayank and Goyal, Pawan},
      booktitle={2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
      title={COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews}, 
      year={2021},
      volume={},
      number={},
      pages={238-241},
      keywords={Deep learning;Annotations;Taxonomy;Bibliometrics;Libraries;Scientometrics;Peer Review;Taxonomy},
      doi={10.1109/JCDL52503.2021.00068}}

      @article{Liu2019RoBERTaAR,
        title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
        author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
        journal={ArXiv},
        year={2019},
        volume={abs/1907.11692},
        url={https://api.semanticscholar.org/CorpusID:198953378}
      }      
    </script>
  </d-bibliography>
  <style>
      d-appendix .citation {
          font-size: 11px;
          line-height: 15px;
          border-left: 1px solid rgba(0, 0, 0, 0.1);
          padding-left: 18px;
          border: 1px solid rgba(0, 0, 0, 0.1);
          background: rgba(0, 0, 0, 0.02);
          padding: 10px 18px;
          border-radius: 3px;
          color: rgba(150, 150, 150, 1);
          overflow: hidden;
          margin-top: -12px;
          white-space: pre-wrap;
          word-wrap: break-word;
      }
  </style>

  <h3 id="citation">Citation</h3>
  <p>For attribution in academic contexts, please cite this work as</p>
  <pre
      class="citation short">Neumann, M. (2025). Structured and Linked OpenReview Reviews and Comments (Version v1) [Data set]. Zenodo.</pre>
  <p>BibTeX citation</p>
  <pre class="citation long">
    @dataset{neumann_2025_15866613,
      author       = {Neumann, Mark},
      title        = {Structured and Linked OpenReview Reviews and Comments},
      month        = jul,
      year         = 2025,
      publisher    = {Zenodo},
      version      = {v1},
      doi          = {10.5281/zenodo.15866613},
      url          = {https://doi.org/10.5281/zenodo.15866613},
    }

</pre>
</d-appendix>
